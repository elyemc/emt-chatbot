import os
import streamlit as st
from pypdf import PdfReader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.chains import RetrievalQA
from langchain.chat_models import ChatOpenAI

# â”€â”€â”€ 1) Grab API key from env â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
openai_api_key = os.getenv("OPENAI_API_KEY")
if not openai_api_key:
    st.error("âš ï¸ Missing OPENAI_API_KEY environment variable. Set it in your shell with:\n\n"
             "export OPENAI_API_KEY=\"sk-...\"")
    st.stop()

# â”€â”€â”€ 2) Streamlit page setup â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.set_page_config(page_title="EMT Chatbot", layout="wide")
st.title("ğŸš‘ EMT Medical Orders Assistant")
st.markdown("Ask a question based on the official medical standing orders.")

# â”€â”€â”€ 3) Load & cache vectorstore â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@st.cache_resource
def load_vectorstore():
    # Load the PDF
    reader = PdfReader("medical_orders.pdf")
    raw_text = ""
    for page in reader.pages:
        raw_text += page.extract_text() or ""

    # Split into chunks
    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
    docs = splitter.create_documents([raw_text])

    # Embed with OpenAI
    embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)
    vect = Chroma.from_documents(docs, embeddings)
    return vect

vectorstore = load_vectorstore()

# â”€â”€â”€ 4) Build the QA chain â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
llm = ChatOpenAI(temperature=0, openai_api_key=openai_api_key)
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=vectorstore.as_retriever(),
    return_source_documents=True
)

# â”€â”€â”€ 5) Chat UI â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
query = st.text_input(
    "ğŸ§  What do you need to know?",
    placeholder="e.g., What is the protocol for allergic reactions?"
)

if query:
    with st.spinner("Searching the standing orders..."):
        result = qa_chain(query)
        st.success("âœ… Response:")
        st.write(result["result"])

        with st.expander("ğŸ“š Sources"):
            for doc in result["source_documents"]:
                snippet = doc.page_content.replace("\n", " ")[:300]
                st.markdown(f"- {snippet}â€¦")